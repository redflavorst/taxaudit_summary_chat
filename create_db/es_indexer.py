from elasticsearch import Elasticsearch, helpers
from datetime import datetime
from typing import List, Dict


def index_findings(es: Elasticsearch, index: str, findings, doc_meta_by_docid, row_finding_maps=None):
    """
    findingsë¥¼ ESì— ì¸ë±ì‹±
    
    Args:
        es: Elasticsearch í´ë¼ì´ì–¸íŠ¸
        index: ì¸ë±ìŠ¤ ì´ë¦„
        findings: finding ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        doc_meta_by_docid: ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        row_finding_maps: row-finding ë§¤í•‘ ë¦¬ìŠ¤íŠ¸ (optional)
    """
    actions = []
    
    # row_finding_mapsì—ì„œ findingë³„ ì—°ê²° ì •ë³´ ì¶”ì¶œ
    finding_to_rows = {}
    finding_to_codes_from_rows = {}
    if row_finding_maps:
        for m in row_finding_maps:
            fid = m["finding_id"]
            rid = m["row_id"]
            if fid not in finding_to_rows:
                finding_to_rows[fid] = []
            finding_to_rows[fid].append(rid)
    
    for idx, f in enumerate(findings, 1):
        meta = doc_meta_by_docid.get(f["doc_id"], {})
        overview_keywords = meta.get("overview_keywords_norm") or []
        
        # row_ids ê°€ì ¸ì˜¤ê¸°
        row_ids = finding_to_rows.get(f["finding_id"], f.get("row_ids", []))
        
        # codes_from_rows ì¶”ì¶œ (ì—°ê²°ëœ rowë“¤ì˜ ì½”ë“œ)
        codes_from_rows = list(set(
            code for code in [f.get("code")] if code
        ))
        
        # item_detailì—ì„œ ê°œí–‰ ì²˜ë¦¬ (ESëŠ” ì¼ë°˜ JSON ë¬¸ìì—´ë§Œ í—ˆìš©)
        item_detail = f.get("item_detail") or ""
        if item_detail:
            # ì´ë¯¸ ê°œí–‰ì´ ìˆë‹¤ë©´ ê·¸ëŒ€ë¡œ, ì—†ë‹¤ë©´ ë³€í™˜ ë¶ˆí•„ìš”
            item_detail = item_detail.strip()
        
        # reason_kw_normì´ ë¬¸ìì—´ì´ë©´ ë°°ì—´ë¡œ ë³€í™˜
        reason_kw_norm = f.get("reason_kw_norm", [])
        if isinstance(reason_kw_norm, str):
            reason_kw_norm = [kw.strip() for kw in reason_kw_norm.split(",") if kw.strip()]
        
        src = {
            "finding_id": f["finding_id"],
            "doc_id": f["doc_id"],
            "doc_title": meta.get("doc_title") or f["doc_id"],
            "finding_order": idx,
            
            # ì§€ì ì‚¬í•­ ë‚´ìš©
            "item": f.get("item"),
            "item_norm": f.get("item_norm"),
            "item_detail": item_detail,
            
            # ì½”ë“œ
            "code": f.get("code"),
            "codes_from_rows": codes_from_rows,
            "code_mismatch": f.get("code_mismatch", False),
            
            # í‚¤ì›Œë“œ
            "reason_kw_norm": reason_kw_norm,
            "overview_keywords": " ".join(overview_keywords) if overview_keywords else "",
            
            # ë¶„ë¥˜
            "industry_sub": meta.get("industry_sub"),
            "domain_tags": meta.get("domain_tags", []),
            "actions": meta.get("actions", []),
            "entities": meta.get("entities", []),
            
            # ì„¹ì…˜
            "sections_present": f.get("sections_present", []),
            "section_spans": f.get("section_spans", []),
            
            # ë²”ìœ„
            "start_line": f.get("start_line"),
            "end_line": f.get("end_line"),
            "start_page": f.get("start_page"),
            "end_page": f.get("end_page"),
            
            # ì—°ê²°
            "row_ids": row_ids,
            "chunk_count": f.get("chunk_count", 0),
            
            # ë©”íƒ€
            "created_at": datetime.utcnow().isoformat() + "Z",
            "extraction_version": "v0.4.0"
        }
        actions.append({"_index": index, "_id": f["finding_id"], "_source": src})
    
    if actions:
        helpers.bulk(es, actions)
        print(f"OK: {len(actions)} findings indexed")


def index_chunks(es: Elasticsearch, index: str, chunks):
    """
    chunksë¥¼ ESì— ì¸ë±ì‹±
    
    Args:
        es: Elasticsearch í´ë¼ì´ì–¸íŠ¸
        index: ì¸ë±ìŠ¤ ì´ë¦„
        chunks: chunk ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    actions = []
    for c in chunks:
        start_line = c.get("start_line")
        end_line = c.get("end_line")
        line_range = None
        if isinstance(start_line, int) and isinstance(end_line, int):
            line_range = {"gte": start_line, "lte": end_line}
        
        text = c.get("text") or ""
        text_norm = c.get("text_norm") or text
        text_raw = c.get("text_raw") or text

        src = {
            "chunk_id": c["chunk_id"],
            "finding_id": c["finding_id"],
            "doc_id": c["doc_id"],
            "section": c.get("section"),
            "section_order": c.get("section_order"),
            "chunk_order": c.get("chunk_order"),
            "code": c.get("code"),
            "item": c.get("item"),
            "item_norm": c.get("item_norm"),
            "page": c.get("page"),
            "start_line": start_line,
            "end_line": end_line,
            "line_range": line_range,
            "text": text.strip() if text else "",
            "text_norm": text_norm.strip() if text_norm else "",
            "text_raw": text_raw.strip() if text_raw else "",
            "meta_line": c.get("meta_line"),
            "extraction_version": c.get("extraction_version", "v0.4.0"),
            "created_at": c.get("created_at"),
        }
        actions.append({"_index": index, "_id": c["chunk_id"], "_source": src})
    
    if actions:
        helpers.bulk(es, actions)
        print(f"OK: {len(actions)} chunks indexed")


def index_laws(es: Elasticsearch, index: str, law_refs: List[Dict]):
    """
    law_referencesë¥¼ ESì— ì¸ë±ì‹±
    
    Args:
        es: Elasticsearch í´ë¼ì´ì–¸íŠ¸
        index: ì¸ë±ìŠ¤ ì´ë¦„ (law_references)
        law_refs: law_reference ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    from es_mappings import LAW_REFERENCES_MAPPING, create_index_if_not_exists
    
    # ì¸ë±ìŠ¤ ìƒì„± (ì—†ìœ¼ë©´)
    create_index_if_not_exists(es, index, LAW_REFERENCES_MAPPING)
    
    actions = []
    for law in law_refs:
        src = {
            "law_id": law["law_id"],
            "finding_id": law.get("finding_id"),
            "doc_id": law["doc_id"],
            "law_type": law.get("law_type"),
            "law_name": law.get("law_name"),
            "law_content": law.get("law_content"),
            "page": law.get("page"),
            "line_number": law.get("line_number"),
            "law_order": law.get("law_order"),
            "extraction_version": law.get("extraction_version", "v0.5.0"),
            "created_at": datetime.utcnow().isoformat() + "Z"
        }
        actions.append({"_index": index, "_id": law["law_id"], "_source": src})
    
    if actions:
        helpers.bulk(es, actions)
        print(f"OK: {len(actions)} law_references indexed")


def main():
    """PostgreSQLì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ Elasticsearchì— ì¬ì¸ë±ì‹±"""
    import psycopg2
    from config import settings
    
    # PostgreSQL ì—°ê²°
    print("ğŸ“¦ PostgreSQLì—ì„œ ë°ì´í„° ë¡œë”© ì¤‘...")
    conn = psycopg2.connect(settings.PG_DSN)
    conn.set_client_encoding('UTF8')
    cur = conn.cursor()
    
    # findings ë¡œë”©
    cur.execute("SELECT * FROM findings ORDER BY doc_id, finding_id")
    findings_rows = cur.fetchall()
    findings_cols = [desc[0] for desc in cur.description]
    findings = [dict(zip(findings_cols, row)) for row in findings_rows]
    print(f"  âœ“ findings: {len(findings)}ê°œ")
    
    # chunks ë¡œë”©
    cur.execute("SELECT * FROM chunks ORDER BY doc_id, finding_id, chunk_id")
    chunks_rows = cur.fetchall()
    chunks_cols = [desc[0] for desc in cur.description]
    chunks = [dict(zip(chunks_cols, row)) for row in chunks_rows]
    print(f"  âœ“ chunks: {len(chunks)}ê°œ")
    
    # documents ë¡œë”©
    cur.execute("SELECT * FROM documents")
    meta_rows = cur.fetchall()
    meta_cols = [desc[0] for desc in cur.description]
    doc_meta_by_docid = {row[0]: dict(zip(meta_cols, row)) for row in meta_rows}
    print(f"  âœ“ documents: {len(doc_meta_by_docid)}ê°œ")
    
    cur.close()
    conn.close()
    
    # Elasticsearch ì—°ê²°
    print("\nğŸ” Elasticsearch ì¸ë±ì‹± ì¤‘...")
    es_kwargs = {"hosts": [settings.ES_URL]}
    if settings.ES_USER and settings.ES_PASSWORD:
        es_kwargs["basic_auth"] = (settings.ES_USER, settings.ES_PASSWORD)
    
    es = Elasticsearch(**es_kwargs)
    
    # ì¸ë±ì‹±
    index_findings(es, "findings", findings, doc_meta_by_docid)
    index_chunks(es, "chunks", chunks)
    
    # í™•ì¸
    findings_count = es.count(index="findings")["count"]
    chunks_count = es.count(index="chunks")["count"]
    
    print(f"\nâœ… ì¸ë±ì‹± ì™„ë£Œ!")
    print(f"  - findings: {findings_count}ê°œ")
    print(f"  - chunks: {chunks_count}ê°œ")


if __name__ == "__main__":
    main()
