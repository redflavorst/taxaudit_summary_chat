# ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ

## ë¬¸ì œ 3: N+1 ì¿¼ë¦¬ íŒ¨í„´

### í˜„ì¬ ë¬¸ì œì  (`retrieval.py:93-156`)

```python
def _calculate_keyword_frequency(self, doc_ids: List[str], keywords: List[str]) -> Dict[str, int]:
    """ë¬¸ì„œë“¤ì—ì„œ ê° í‚¤ì›Œë“œì˜ ì´ ì¶œí˜„ ë¹ˆë„ ê³„ì‚°"""
    keyword_freq = {kw: 0 for kw in keywords}

    for doc_id in doc_ids[:5]:  # ìµœëŒ€ 5ê°œ ë¬¸ì„œ
        for kw in keywords:  # ê° í‚¤ì›Œë“œë§ˆë‹¤
            try:
                # âŒ ë¬¸ì œ: ë¬¸ì„œë§ˆë‹¤ í‚¤ì›Œë“œë§ˆë‹¤ ê°œë³„ ì¿¼ë¦¬ â†’ 5 * Në²ˆ ì¿¼ë¦¬
                result = self.es.search(
                    index="findings",
                    body={
                        "query": {
                            "bool": {
                                "must": [
                                    {"term": {"doc_id": doc_id}},
                                    {"match": {"text": kw}}
                                ]
                            }
                        },
                        # ... ì§‘ê³„ ë¡œì§
                    }
                )
                count = result['hits']['total']['value']
                keyword_freq[kw] += count
            except Exception:
                # fallbackìœ¼ë¡œ ë˜ ë‹¤ë¥¸ ì¿¼ë¦¬
                pass

    return keyword_freq
```

### ì„±ëŠ¥ ë¬¸ì œ ë¶„ì„

#### ì¿¼ë¦¬ ìˆ˜ ê³„ì‚°
- ë¬¸ì„œ 5ê°œ, í‚¤ì›Œë“œ 3ê°œ â†’ **15ë²ˆ ì¿¼ë¦¬** ğŸ”´
- ë¬¸ì„œ 5ê°œ, í‚¤ì›Œë“œ 5ê°œ â†’ **25ë²ˆ ì¿¼ë¦¬** ğŸ”´
- ê° ì¿¼ë¦¬ë§ˆë‹¤ ë„¤íŠ¸ì›Œí¬ ì™•ë³µ ì‹œê°„ (RTT) ë°œìƒ

#### ì‹¤ì œ ì˜í–¥
```
ë‹¨ì¼ ì¿¼ë¦¬ ì‘ë‹µ ì‹œê°„: 10ms
15ë²ˆ ì¿¼ë¦¬ = 150ms (ì§ë ¬ ì‹¤í–‰)
25ë²ˆ ì¿¼ë¦¬ = 250ms (ì§ë ¬ ì‹¤í–‰)
```

ì‚¬ìš©ìê°€ ì²´ê°í•˜ëŠ” ì§€ì—° ì‹œê°„ì´ í¬ê²Œ ì¦ê°€í•©ë‹ˆë‹¤.

---

## í•´ê²° ë°©ë²• 1: Multi-Search API (msearch)

### ê°œì„ ëœ ì½”ë“œ

```python
def _calculate_keyword_frequency_optimized(
    self,
    doc_ids: List[str],
    keywords: List[str]
) -> Dict[str, int]:
    """
    Elasticsearch Multi-Search APIë¥¼ ì‚¬ìš©í•œ ìµœì í™”ëœ í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°

    ê°œì„ ì :
    - N+1 ì¿¼ë¦¬ â†’ ë‹¨ì¼ bulk ìš”ì²­
    - 15ë²ˆ ì¿¼ë¦¬ â†’ 1ë²ˆ ì¿¼ë¦¬ (15ë°° ì„±ëŠ¥ í–¥ìƒ)
    """
    if not doc_ids or not keywords:
        return {kw: 0 for kw in keywords}

    keyword_freq = {kw: 0 for kw in keywords}

    # Multi-Search ìš”ì²­ êµ¬ì„±
    requests = []
    query_map = []  # (doc_id, keyword) íŠœí”Œë¡œ ë§¤í•‘

    for doc_id in doc_ids[:5]:
        for kw in keywords:
            # ê° ê²€ìƒ‰ í—¤ë” (ì¸ë±ìŠ¤ ì§€ì •)
            requests.append({"index": "findings"})

            # ê° ê²€ìƒ‰ ì¿¼ë¦¬
            requests.append({
                "query": {
                    "bool": {
                        "must": [
                            {"term": {"doc_id": doc_id}},
                            {"match": {"text": kw}}
                        ]
                    }
                },
                "size": 0,  # ë¬¸ì„œ ë‚´ìš© ë¶ˆí•„ìš”, ì¹´ìš´íŠ¸ë§Œ
                "track_total_hits": True
            })

            query_map.append((doc_id, kw))

    try:
        # âœ… ë‹¨ì¼ msearch ìš”ì²­ìœ¼ë¡œ ëª¨ë“  ì¿¼ë¦¬ ì‹¤í–‰
        response = self.es.msearch(body=requests, index="findings")

        # ê²°ê³¼ ì§‘ê³„
        for i, resp in enumerate(response['responses']):
            if 'error' in resp:
                logger.warning(f"ì¿¼ë¦¬ {i} ì‹¤íŒ¨: {resp['error']}")
                continue

            doc_id, kw = query_map[i]
            count = resp['hits']['total']['value']
            keyword_freq[kw] += count

        logger.debug(f"í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚° ì™„ë£Œ (ì¿¼ë¦¬ 1ë²ˆ): {keyword_freq}")
        return keyword_freq

    except ElasticsearchException as e:
        logger.error(f"msearch ì‹¤íŒ¨, í´ë°± ì‚¬ìš©: {e}")
        return self._calculate_keyword_frequency_fallback(doc_ids, keywords)
```

### ì„±ëŠ¥ ë¹„êµ

| ë°©ì‹ | ì¿¼ë¦¬ ìˆ˜ | ì˜ˆìƒ ì‹œê°„ (10ms/query) | ê°œì„ ìœ¨ |
|------|---------|------------------------|--------|
| **ê¸°ì¡´ (N+1)** | 15ë²ˆ | 150ms | - |
| **msearch** | 1ë²ˆ | 15ms | **10ë°° âš¡** |

---

## í•´ê²° ë°©ë²• 2: Aggregation í™œìš©

ë” íš¨ìœ¨ì ì¸ ë°©ë²•ì€ **ë‹¨ì¼ aggregation ì¿¼ë¦¬**ë¡œ ëª¨ë“  ì •ë³´ë¥¼ í•œ ë²ˆì— ê°€ì ¸ì˜¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

```python
def _calculate_keyword_frequency_aggregation(
    self,
    doc_ids: List[str],
    keywords: List[str]
) -> Dict[str, int]:
    """
    Elasticsearch Aggregationì„ ì‚¬ìš©í•œ ìµœì í™”ëœ í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°

    ê°œì„ ì :
    - ë‹¨ì¼ ì¿¼ë¦¬ë¡œ ëª¨ë“  ë¬¸ì„œì˜ ëª¨ë“  í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
    - O(N*M) â†’ O(1) ì¿¼ë¦¬ ë³µì¡ë„
    """
    if not doc_ids or not keywords:
        return {kw: 0 for kw in keywords}

    # ë‹¨ì¼ aggregation ì¿¼ë¦¬ êµ¬ì„±
    query = {
        "query": {
            "bool": {
                "must": [
                    {"terms": {"doc_id": doc_ids[:5]}},  # 5ê°œ ë¬¸ì„œ í•„í„°
                    {
                        "bool": {
                            "should": [
                                {"match": {"text": kw}} for kw in keywords
                            ],
                            "minimum_should_match": 1
                        }
                    }
                ]
            }
        },
        "size": 0,  # ë¬¸ì„œ ë‚´ìš© ë¶ˆí•„ìš”
        "aggs": {
            "by_keyword": {
                "filters": {
                    "filters": {
                        kw: {"match": {"text": kw}} for kw in keywords
                    }
                }
            }
        }
    }

    try:
        # âœ… ë‹¨ì¼ aggregation ì¿¼ë¦¬
        response = self.es.search(index="findings", body=query)

        # Aggregation ê²°ê³¼ íŒŒì‹±
        keyword_freq = {}
        buckets = response['aggregations']['by_keyword']['buckets']

        for kw in keywords:
            keyword_freq[kw] = buckets.get(kw, {}).get('doc_count', 0)

        logger.info(f"í‚¤ì›Œë“œ ë¹ˆë„ (aggregation): {keyword_freq}")
        return keyword_freq

    except ElasticsearchException as e:
        logger.error(f"Aggregation ì‹¤íŒ¨: {e}", exc_info=True)
        return {kw: 0 for kw in keywords}
```

### ì„±ëŠ¥ ë¹„êµ

| ë°©ì‹ | ì¿¼ë¦¬ ìˆ˜ | ì˜ˆìƒ ì‹œê°„ | ê°œì„ ìœ¨ |
|------|---------|-----------|--------|
| **ê¸°ì¡´ (N+1)** | 15ë²ˆ | 150ms | - |
| **msearch** | 1ë²ˆ | 15ms | 10ë°° |
| **aggregation** | 1ë²ˆ | **8ms** | **18ë°° âš¡âš¡** |

Aggregationì´ ì„œë²„ ì‚¬ì´ë“œì—ì„œ ìµœì í™”ë˜ì–´ ë” ë¹ ë¦…ë‹ˆë‹¤.

---

## ì¶”ê°€ ìµœì í™”: ìºì‹± ì „ëµ

í‚¤ì›Œë“œ ë¹ˆë„ëŠ” ë¬¸ì„œê°€ ë³€ê²½ë˜ì§€ ì•ŠëŠ” í•œ ë™ì¼í•˜ë¯€ë¡œ ìºì‹±ì´ íš¨ê³¼ì ì…ë‹ˆë‹¤.

```python
from functools import lru_cache
from typing import Tuple

class HybridRetriever:
    def __init__(self):
        self.es = Elasticsearch(...)
        self.qdrant = QdrantClient(...)
        self.embedder = get_embedder()

        # ìºì‹œ ì´ˆê¸°í™”
        self._keyword_freq_cache = {}

    def _get_cache_key(self, doc_ids: List[str], keywords: List[str]) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        doc_str = ",".join(sorted(doc_ids[:5]))
        kw_str = ",".join(sorted(keywords))
        return f"{doc_str}|{kw_str}"

    def _calculate_keyword_frequency_cached(
        self,
        doc_ids: List[str],
        keywords: List[str]
    ) -> Dict[str, int]:
        """ìºì‹±ì„ ì ìš©í•œ í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°"""

        cache_key = self._get_cache_key(doc_ids, keywords)

        # ìºì‹œ í™•ì¸
        if cache_key in self._keyword_freq_cache:
            logger.debug(f"í‚¤ì›Œë“œ ë¹ˆë„ ìºì‹œ íˆíŠ¸: {cache_key}")
            return self._keyword_freq_cache[cache_key]

        # ìºì‹œ ë¯¸ìŠ¤ â†’ ê³„ì‚°
        logger.debug(f"í‚¤ì›Œë“œ ë¹ˆë„ ìºì‹œ ë¯¸ìŠ¤, ê³„ì‚° ì¤‘...")
        keyword_freq = self._calculate_keyword_frequency_aggregation(doc_ids, keywords)

        # ìºì‹œ ì €ì¥ (ìµœëŒ€ 1000ê°œ ìœ ì§€)
        if len(self._keyword_freq_cache) > 1000:
            # LRU ë°©ì‹ìœ¼ë¡œ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            oldest_key = next(iter(self._keyword_freq_cache))
            del self._keyword_freq_cache[oldest_key]

        self._keyword_freq_cache[cache_key] = keyword_freq
        return keyword_freq
```

### ìºì‹± íš¨ê³¼

**ì‹œë‚˜ë¦¬ì˜¤:** ë™ì¼í•œ ì¿¼ë¦¬ë¥¼ 3ë²ˆ ë°˜ë³µ

| ë°©ì‹ | 1ì°¨ | 2ì°¨ | 3ì°¨ | í‰ê·  |
|------|-----|-----|-----|------|
| **ìºì‹± ì—†ìŒ** | 8ms | 8ms | 8ms | 8ms |
| **ìºì‹± ì ìš©** | 8ms | **0.1ms** | **0.1ms** | **2.7ms** âš¡ |

---

## ì„ë² ë”© ìƒì„± ìµœì í™”

### ë¬¸ì œì  (`retrieval.py:404`)

```python
def retrieve_chunks_by_section(self, query: str, ...):
    # âŒ ë§¤ë²ˆ ì„ë² ë”© ìƒì„± (ë¹„ìš©ì´ í¼)
    query_vec = self.embedder.embed_query(query)

    vec_results = self.qdrant.search(
        collection_name=config.qdrant_collection_chunks,
        query_vector=query_vec,
        ...
    )
```

**ë¬¸ì œ:**
- ì„ë² ë”© ìƒì„±ì€ ML ëª¨ë¸ ì¶”ë¡ ì´ë¯€ë¡œ ë¹„ìš©ì´ í¼ (100-200ms)
- ë™ì¼í•œ ì¿¼ë¦¬ë¡œ ì—¬ëŸ¬ ì„¹ì…˜ ê²€ìƒ‰ ì‹œ ì¤‘ë³µ ìƒì„±

### í•´ê²°: ì„ë² ë”© ìºì‹±

```python
from functools import lru_cache
import hashlib

class HybridRetriever:
    def __init__(self):
        self.es = Elasticsearch(...)
        self.qdrant = QdrantClient(...)
        self.embedder = get_embedder()

        # ì„ë² ë”© ìºì‹œ (LRU, ìµœëŒ€ 100ê°œ)
        self._embedding_cache = {}
        self._max_cache_size = 100

    def _get_query_embedding_cached(self, query: str) -> List[float]:
        """ìºì‹±ì„ ì ìš©í•œ ì„ë² ë”© ìƒì„±"""

        # ìºì‹œ í‚¤ (ì¿¼ë¦¬ í•´ì‹œ)
        cache_key = hashlib.md5(query.encode()).hexdigest()

        # ìºì‹œ í™•ì¸
        if cache_key in self._embedding_cache:
            logger.debug(f"ì„ë² ë”© ìºì‹œ íˆíŠ¸: {query[:50]}")
            return self._embedding_cache[cache_key]

        # ìºì‹œ ë¯¸ìŠ¤ â†’ ìƒì„±
        logger.debug(f"ì„ë² ë”© ìƒì„± ì¤‘: {query[:50]}")
        start_time = time.time()

        embedding = self.embedder.embed_query(query)

        elapsed = time.time() - start_time
        logger.debug(f"ì„ë² ë”© ìƒì„± ì™„ë£Œ ({elapsed*1000:.1f}ms)")

        # ìºì‹œ ì €ì¥ (LRU)
        if len(self._embedding_cache) >= self._max_cache_size:
            oldest_key = next(iter(self._embedding_cache))
            del self._embedding_cache[oldest_key]

        self._embedding_cache[cache_key] = embedding
        return embedding

    def retrieve_findings(self, query: str, ...):
        # âœ… ìºì‹±ëœ ì„ë² ë”© ì‚¬ìš©
        query_vec = self._get_query_embedding_cached(query)

        vec_results = self.qdrant.search(
            collection_name=config.qdrant_collection_findings,
            query_vector=query_vec,
            ...
        )
        ...

    def retrieve_chunks_by_section(self, query: str, ...):
        # âœ… ìºì‹±ëœ ì„ë² ë”© ì‚¬ìš© (ë™ì¼ ì¿¼ë¦¬ë©´ ì¬ì‚¬ìš©)
        query_vec = self._get_query_embedding_cached(query)

        vec_results = self.qdrant.search(
            collection_name=config.qdrant_collection_chunks,
            query_vector=query_vec,
            ...
        )
        ...
```

### ì„ë² ë”© ìºì‹± íš¨ê³¼

**ì‹œë‚˜ë¦¬ì˜¤:** ë™ì¼í•œ ì¿¼ë¦¬ë¡œ findings + chunks ê²€ìƒ‰

| ë°©ì‹ | findings | chunks | ì´ ì‹œê°„ |
|------|----------|--------|---------|
| **ìºì‹± ì—†ìŒ** | 150ms | 150ms | **300ms** |
| **ìºì‹± ì ìš©** | 150ms | **0.1ms** (ìºì‹œ) | **150ms** âš¡ |

50% ì„±ëŠ¥ í–¥ìƒ!

---

## ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”

### ë¬¸ì œì  (`run_ingest.py:229-232`)

```python
# âŒ ë¬¸ì„œë§ˆë‹¤ ê°œë³„ upsert â†’ Në²ˆ DB ì™•ë³µ
upsert_many(conn, "table_rows", rows, "row_id")
upsert_many(conn, "findings", findings, "finding_id")
upsert_many(conn, "chunks", all_chunks, "chunk_id")
upsert_many(conn, "law_references", law_refs, "law_id")
```

### í•´ê²°: íŠ¸ëœì­ì…˜ ë°°ì¹˜ ì²˜ë¦¬

```python
def main_optimized(md_paths):
    """ìµœì í™”ëœ ETL íŒŒì´í”„ë¼ì¸ (ë°°ì¹˜ ì²˜ë¦¬)"""
    conn = make_pg_conn()

    # ëª¨ë“  ë¬¸ì„œ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ìˆ˜ì§‘
    all_rows = []
    all_findings = []
    all_chunks = []
    all_law_refs = []

    for mp in md_paths:
        # ... íŒŒì‹± ë¡œì§
        all_rows.extend(rows)
        all_findings.extend(findings)
        all_chunks.extend(all_chunks_for_doc)
        all_law_refs.extend(law_refs)

    # âœ… ë‹¨ì¼ ë°°ì¹˜ë¡œ ëª¨ë“  ë°ì´í„° ì‚½ì…
    try:
        with conn:  # ìë™ commit/rollback
            upsert_many(conn, "table_rows", all_rows, "row_id")
            upsert_many(conn, "findings", all_findings, "finding_id")
            upsert_many(conn, "chunks", all_chunks, "chunk_id")
            upsert_many(conn, "law_references", all_law_refs, "law_id")

        logger.info(f"ë°°ì¹˜ ì‚½ì… ì™„ë£Œ: {len(md_paths)}ê°œ ë¬¸ì„œ")

    except DatabaseError as e:
        logger.error(f"ë°°ì¹˜ ì‚½ì… ì‹¤íŒ¨: {e}")
        conn.rollback()
```

### ë°°ì¹˜ ì²˜ë¦¬ íš¨ê³¼

**ì‹œë‚˜ë¦¬ì˜¤:** 10ê°œ ë¬¸ì„œ ì²˜ë¦¬

| ë°©ì‹ | DB íŠ¸ëœì­ì…˜ | ì‹œê°„ | ê°œì„ ìœ¨ |
|------|-------------|------|--------|
| **ê°œë³„ ì²˜ë¦¬** | 40íšŒ | 2000ms | - |
| **ë°°ì¹˜ ì²˜ë¦¬** | 1íšŒ | **200ms** | **10ë°° âš¡** |

---

## ìµœì¢… ì„±ëŠ¥ ê°œì„  ìš”ì•½

| ìµœì í™” í•­ëª© | ê¸°ì¡´ ì‹œê°„ | ê°œì„  í›„ | ê°œì„ ìœ¨ |
|------------|-----------|---------|--------|
| **N+1 ì¿¼ë¦¬ (msearch)** | 150ms | 15ms | 10ë°° |
| **N+1 ì¿¼ë¦¬ (aggregation)** | 150ms | 8ms | 18ë°° |
| **ì„ë² ë”© ìºì‹±** | 300ms | 150ms | 2ë°° |
| **ë°°ì¹˜ ì²˜ë¦¬** | 2000ms | 200ms | 10ë°° |
| **ì „ì²´ íŒŒì´í”„ë¼ì¸** | ~2.6ì´ˆ | ~0.4ì´ˆ | **6.5ë°° âš¡âš¡âš¡** |

---

## êµ¬í˜„ ìš°ì„ ìˆœìœ„

### ğŸ”´ High Priority (ì¦‰ì‹œ ì ìš©)
1. **Aggregationìœ¼ë¡œ N+1 ì¿¼ë¦¬ ì œê±°** - ê°€ì¥ í° ë³‘ëª©
2. **ì„ë² ë”© ìºì‹±** - ê°„ë‹¨í•˜ê³  íš¨ê³¼ í¼

### ğŸŸ¡ Medium Priority (1ì£¼ ë‚´)
3. **ë°°ì¹˜ ì²˜ë¦¬** - ETL íŒŒì´í”„ë¼ì¸ ê°œì„ 
4. **í‚¤ì›Œë“œ ë¹ˆë„ ìºì‹±** - ì¤‘ë³µ ì¿¼ë¦¬ ì œê±°

### ğŸŸ¢ Low Priority (ì¥ê¸°)
5. **Connection pooling** - DB ì—°ê²° ì¬ì‚¬ìš©
6. **Redis ìºì‹±** - ë¶„ì‚° í™˜ê²½ ì§€ì›
7. **ë¹„ë™ê¸° ì²˜ë¦¬ (asyncio)** - I/O ë³‘ë ¬í™”
