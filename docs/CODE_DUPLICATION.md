# ì½”ë“œ ì¤‘ë³µ ì œê±° ê°€ì´ë“œ

## ë¬¸ì œ 4: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë¡œì§ ì¤‘ë³µ

### í˜„ì¬ ë¬¸ì œì 

`retrieval.py`ì—ì„œ **ê±°ì˜ ë™ì¼í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ íŒ¨í„´**ì´ ë‘ ê³³ì— ë°˜ë³µë©ë‹ˆë‹¤:

1. **`retrieve_findings()`** (157-370ì¤„) - Findings ê²€ìƒ‰
2. **`retrieve_chunks_by_section()`** (372-482ì¤„) - Chunks ê²€ìƒ‰

ë‘ ë©”ì„œë“œ ëª¨ë‘:
- Elasticsearch BM25 ê²€ìƒ‰
- Qdrant ë²¡í„° ê²€ìƒ‰
- RRF ìœµí•©
- ê²°ê³¼ ë§¤í•‘

ë™ì¼í•œ íŒ¨í„´ì„ ìˆ˜í–‰í•˜ì§€ë§Œ **213ì¤„ì˜ ì½”ë“œê°€ ì¤‘ë³µ**ë©ë‹ˆë‹¤.

---

## ì¤‘ë³µ ì½”ë“œ ë¶„ì„

### retrieve_findings() êµ¬ì¡°

```python
def retrieve_findings(self, query: str, filters, expansion, top_n) -> tuple:
    # 1. êµì§‘í•© ë¬¸ì„œ í•„í„°ë§ (í‚¤ì›Œë“œ ê¸°ë°˜)
    target_doc_ids = None
    if expansion and expansion.get("must_have"):
        # ... í‚¤ì›Œë“œë³„ ë¬¸ì„œ ê²€ìƒ‰
        # ... êµì§‘í•©/í•©ì§‘í•© ê³„ì‚°

    # 2. ES ì¿¼ë¦¬ êµ¬ì„±
    es_query = {"bool": {...}}
    if target_doc_ids:
        must_clauses.append({"terms": {"doc_id": target_doc_ids}})

    # 3. ES ê²€ìƒ‰
    es_results = self.es.search(
        index="findings",
        body={"query": es_query, "size": config.findings_top_k_es}
    )["hits"]["hits"]

    # 4. Qdrant ë²¡í„° ê²€ìƒ‰
    query_vec = self.embedder.embed_query(query)
    vec_results = self.qdrant.search(
        collection_name=config.qdrant_collection_findings,
        query_vector=query_vec,
        ...
    )

    # 5. RRF ìœµí•©
    merged = self._rrf_merge(es_results, vec_results, k=60)[:top_n]

    # 6. FindingHit ê°ì²´ë¡œ ë§¤í•‘
    findings = []
    for hit in merged:
        findings.append(FindingHit(...))

    return findings, target_doc_ids, keyword_freq
```

### retrieve_chunks_by_section() êµ¬ì¡°

```python
def retrieve_chunks_by_section(self, query: str, section, finding_ids, filters, top_n) -> List:
    # 1. ES ì¿¼ë¦¬ êµ¬ì„± (ì„¹ì…˜ í•„í„° ì¶”ê°€)
    must_clauses = [
        {"multi_match": {"query": query, "fields": ["text^2", "text_norm", "item"]}},
        {"term": {"section": section}},
        {"terms": {"finding_id": finding_ids}}
    ]

    # 2. ES ê²€ìƒ‰
    es_results = self.es.search(
        index="chunks",
        body={"query": {"bool": {"must": must_clauses}}, "size": config.chunks_top_k_es}
    )["hits"]["hits"]

    # 3. Qdrant ë²¡í„° ê²€ìƒ‰
    query_vec = self.embedder.embed_query(query)
    vec_results = self.qdrant.search(
        collection_name=config.qdrant_collection_chunks,
        query_vector=query_vec,
        ...
    )

    # 4. RRF ìœµí•©
    merged = self._rrf_merge(es_results, vec_results, k=60)[:top_n]

    # 5. ChunkHit ê°ì²´ë¡œ ë§¤í•‘ + ESì—ì„œ text ê°€ì ¸ì˜¤ê¸°
    chunks = []
    for hit in merged:
        # ... ES fallback ë¡œì§
        chunks.append(ChunkHit(...))

    return chunks
```

### ì¤‘ë³µ íŒ¨í„´ ì •ë¦¬

| ë‹¨ê³„ | retrieve_findings | retrieve_chunks_by_section | ì¤‘ë³µ ì—¬ë¶€ |
|------|-------------------|----------------------------|-----------|
| 1. ì¿¼ë¦¬ êµ¬ì„± | âœ… | âœ… | âœ… ë™ì¼ |
| 2. ES ê²€ìƒ‰ | âœ… | âœ… | âœ… ë™ì¼ |
| 3. Qdrant ê²€ìƒ‰ | âœ… | âœ… | âœ… ë™ì¼ |
| 4. RRF ìœµí•© | âœ… | âœ… | âœ… ë™ì¼ |
| 5. ê²°ê³¼ ë§¤í•‘ | FindingHit | ChunkHit | âŒ ì°¨ì´ |

**ê²°ë¡ :** 90% ì´ìƒì˜ ë¡œì§ì´ ë™ì¼í•˜ë©°, 5% ì°¨ì´(ê²°ê³¼ ë§¤í•‘)ë§Œ ì¡´ì¬í•©ë‹ˆë‹¤.

---

## í•´ê²° ë°©ë²•: ê³µí†µ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë©”ì„œë“œ ì¶”ì¶œ

### ë¦¬íŒ©í† ë§ ì „ëµ

1. **ê³µí†µ ë¡œì§ ì¶”ì¶œ**: `_hybrid_search()` ë©”ì„œë“œ ìƒì„±
2. **ì°¨ì´ì  íŒŒë¼ë¯¸í„°í™”**: ì¸ë±ìŠ¤ëª…, ì»¬ë ‰ì…˜ëª…, ë§¤í•‘ í•¨ìˆ˜
3. **ì¬ì‚¬ìš©**: `retrieve_findings()`ì™€ `retrieve_chunks_by_section()`ì—ì„œ í˜¸ì¶œ

---

## ë¦¬íŒ©í† ë§ ì½”ë“œ

### 1ë‹¨ê³„: ê³µí†µ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë©”ì„œë“œ

```python
from typing import List, Dict, Any, Optional, Callable, TypeVar
from dataclasses import dataclass

T = TypeVar('T')  # FindingHit ë˜ëŠ” ChunkHit


@dataclass
class HybridSearchConfig:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„¤ì •"""
    es_index: str
    qdrant_collection: str
    es_top_k: int
    vec_top_k: int
    rrf_k: int = 60
    score_threshold: float = 0.35


class HybridRetriever:
    def __init__(self):
        self.es = Elasticsearch(...)
        self.qdrant = QdrantClient(...)
        self.embedder = get_embedder()

    def _hybrid_search(
        self,
        query: str,
        es_query: Dict[str, Any],
        search_config: HybridSearchConfig,
        qdrant_filter: Optional[Filter] = None,
        result_mapper: Optional[Callable[[Dict], T]] = None,
        use_vector: bool = True,
        top_n: int = 100
    ) -> List[T]:
        """
        ê³µí†µ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë¡œì§

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            es_query: Elasticsearch ì¿¼ë¦¬ (bool ì¿¼ë¦¬)
            search_config: ê²€ìƒ‰ ì„¤ì • (ì¸ë±ìŠ¤, ì»¬ë ‰ì…˜, top_k ë“±)
            qdrant_filter: Qdrant í•„í„° (ì„ íƒ)
            result_mapper: ê²°ê³¼ ë§¤í•‘ í•¨ìˆ˜ (Dict â†’ FindingHit/ChunkHit)
            use_vector: ë²¡í„° ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€
            top_n: ìµœì¢… ë°˜í™˜ ê°œìˆ˜

        Returns:
            ë§¤í•‘ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (FindingHit ë˜ëŠ” ChunkHit)
        """

        # 1. Elasticsearch ê²€ìƒ‰
        logger.debug(f"ES ê²€ìƒ‰ ì‹œì‘: index={search_config.es_index}, top_k={search_config.es_top_k}")

        try:
            es_results = self.es.search(
                index=search_config.es_index,
                body={
                    "query": es_query,
                    "size": search_config.es_top_k,
                    "_source": True
                }
            )["hits"]["hits"]

            logger.info(f"ES ê²€ìƒ‰ ì™„ë£Œ: {len(es_results)}ê°œ ê²°ê³¼")

        except ElasticsearchException as e:
            logger.error(f"ES ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            es_results = []

        # 2. Qdrant ë²¡í„° ê²€ìƒ‰ (ì˜µì…˜)
        vec_results = []
        if use_vector:
            logger.debug(f"Qdrant ê²€ìƒ‰ ì‹œì‘: collection={search_config.qdrant_collection}")

            try:
                query_vec = self._get_query_embedding_cached(query)

                vec_results = self.qdrant.search(
                    collection_name=search_config.qdrant_collection,
                    query_vector=query_vec,
                    query_filter=qdrant_filter,
                    limit=search_config.vec_top_k,
                    search_params=SearchParams(
                        exact=False,
                        hnsw_ef=config.qdrant_ef_search
                    ),
                    score_threshold=search_config.score_threshold
                )

                logger.info(f"Qdrant ê²€ìƒ‰ ì™„ë£Œ: {len(vec_results)}ê°œ ê²°ê³¼")

            except QdrantError as e:
                logger.error(f"Qdrant ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                vec_results = []

        # 3. RRF ìœµí•©
        if vec_results:
            merged = self._rrf_merge(es_results, vec_results, k=search_config.rrf_k)[:top_n]
            logger.info(f"RRF ìœµí•© ì™„ë£Œ: ES {len(es_results)} + Qdrant {len(vec_results)} â†’ {len(merged)}")
        else:
            # ë²¡í„° ê²€ìƒ‰ ì—†ìŒ ë˜ëŠ” ì‹¤íŒ¨ â†’ ES ê²°ê³¼ë§Œ ì‚¬ìš©
            merged = es_results[:top_n]
            logger.info(f"BM25ë§Œ ì‚¬ìš©: {len(merged)}ê°œ ê²°ê³¼")

        # 4. ê²°ê³¼ ë§¤í•‘ (FindingHit ë˜ëŠ” ChunkHit)
        if result_mapper:
            return [result_mapper(hit) for hit in merged]
        else:
            # ë§¤í¼ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
            return merged
```

---

### 2ë‹¨ê³„: retrieve_findings() ë¦¬íŒ©í† ë§

```python
def retrieve_findings(
    self,
    query: str,
    filters: Optional[Dict[str, Any]] = None,
    expansion: Optional[Dict[str, Any]] = None,
    top_n: int = 30
) -> tuple[List[FindingHit], Optional[List[str]], Optional[Dict[str, int]]]:
    """
    Findings í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë¦¬íŒ©í† ë§ ë²„ì „)

    ë°˜í™˜ê°’:
        (findings, target_doc_ids, keyword_freq)
    """

    # 1. êµì§‘í•© ë¬¸ì„œ í•„í„°ë§ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    target_doc_ids = None
    keyword_freq = None

    if expansion and expansion.get("must_have"):
        # ... êµì§‘í•©/í•©ì§‘í•© ë¡œì§ (ë™ì¼)
        target_doc_ids, keyword_freq = self._filter_docs_by_keywords(expansion["must_have"])

    # 2. ES ì¿¼ë¦¬ êµ¬ì„±
    es_query = self._build_findings_query(query, expansion, filters, target_doc_ids)

    # 3. Qdrant í•„í„° êµ¬ì„±
    qdrant_filter = self._build_qdrant_filter(filters)

    # 4. ê²€ìƒ‰ ì„¤ì •
    search_config = HybridSearchConfig(
        es_index="findings",
        qdrant_collection=config.qdrant_collection_findings,
        es_top_k=config.findings_top_k_es,
        vec_top_k=config.findings_top_k_vec,
        rrf_k=config.findings_rrf_k,
        score_threshold=0.65 if len(expansion.get("must_have", [])) >= 2 else 0.35
    )

    # 5. ê²°ê³¼ ë§¤í•‘ í•¨ìˆ˜
    def map_to_finding_hit(hit: Dict) -> FindingHit:
        source = hit.get("_source", {})
        if not source and "vec_hit" in hit:
            source = hit["vec_hit"].payload

        score = hit.get("rrf_score", hit.get("_score", 0.0))

        return FindingHit(
            finding_id=source.get("finding_id", hit["_id"]),
            doc_id=source.get("doc_id", ""),
            item=source.get("item"),
            item_detail=source.get("item_detail"),
            code=source.get("code"),
            score_combined=score
        )

    # âœ… ê³µí†µ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë©”ì„œë“œ í˜¸ì¶œ
    use_vector = len(expansion.get("must_have", [])) >= 2 if expansion else True

    findings = self._hybrid_search(
        query=query,
        es_query=es_query,
        search_config=search_config,
        qdrant_filter=qdrant_filter,
        result_mapper=map_to_finding_hit,
        use_vector=use_vector,
        top_n=top_n
    )

    # ìŠ¤ì½”ì–´ í•„í„°ë§ (êµì§‘í•©ì¼ ë•Œë§Œ)
    if target_doc_ids and findings:
        score_threshold = findings[0].score_combined * 0.5
        findings = [f for f in findings if f.score_combined >= score_threshold][:top_n]

    return findings, target_doc_ids, keyword_freq


def _build_findings_query(
    self,
    query: str,
    expansion: Optional[Dict],
    filters: Optional[Dict],
    target_doc_ids: Optional[List[str]]
) -> Dict:
    """Findings ES ì¿¼ë¦¬ êµ¬ì„± (í—¬í¼ ë©”ì„œë“œ)"""

    should_clauses = []
    must_clauses = []

    # ì¿¼ë¦¬ í™•ì¥ ì ìš©
    if expansion and expansion.get("must_have"):
        must_keywords = expansion["must_have"]
        should_keywords = expansion.get("should_have", []) + expansion.get("related_terms", [])
        boost_weights = expansion.get("boost_weights", {})

        for kw in must_keywords:
            boost = boost_weights.get(kw, 3.0)
            should_clauses.append({
                "multi_match": {
                    "query": kw,
                    "fields": [f"item^{boost}", f"reason_kw_norm^{boost*0.8}"]
                }
            })

        for kw in should_keywords:
            boost = boost_weights.get(kw, 1.5)
            should_clauses.append({
                "multi_match": {
                    "query": kw,
                    "fields": [f"item^{boost}", f"reason_kw_norm^{boost*0.8}"]
                }
            })

    else:
        # ê¸°ë³¸ ì¿¼ë¦¬
        must_clauses.append({
            "multi_match": {"query": query, "fields": ["item^2", "reason_kw_norm", "item_detail"]}
        })

    # ë¬¸ì„œ í•„í„°
    if target_doc_ids:
        must_clauses.append({"terms": {"doc_id": target_doc_ids}})

    # ë©”íƒ€ í•„í„°
    if filters:
        if filters.get("code"):
            must_clauses.append({"terms": {"code": filters["code"]}})
        if filters.get("industry_sub"):
            must_clauses.append({"terms": {"industry_sub": filters["industry_sub"]}})

    # ìµœì¢… ì¿¼ë¦¬ êµ¬ì„±
    es_query = {"bool": {}}
    if must_clauses:
        es_query["bool"]["must"] = must_clauses
    if should_clauses:
        es_query["bool"]["should"] = should_clauses
        if not target_doc_ids:
            es_query["bool"]["minimum_should_match"] = 1

    return es_query
```

---

### 3ë‹¨ê³„: retrieve_chunks_by_section() ë¦¬íŒ©í† ë§

```python
def retrieve_chunks_by_section(
    self,
    query: str,
    section: str,
    finding_ids: List[str],
    filters: Optional[Dict[str, Any]] = None,
    top_n: int = 300
) -> List[ChunkHit]:
    """
    Chunks ì„¹ì…˜ë³„ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë¦¬íŒ©í† ë§ ë²„ì „)
    """

    # 1. ES ì¿¼ë¦¬ êµ¬ì„±
    must_clauses = [
        {"multi_match": {"query": query, "fields": ["text^2", "text_norm", "item"]}},
        {"term": {"section": section}},
        {"terms": {"finding_id": finding_ids}}
    ]

    if filters:
        if filters.get("code"):
            must_clauses.append({"terms": {"code": filters["code"]}})
        if filters.get("doc_id"):
            must_clauses.append({"terms": {"doc_id": filters["doc_id"]}})

    es_query = {"bool": {"must": must_clauses}}

    # 2. Qdrant í•„í„° êµ¬ì„±
    qdrant_filter = Filter(
        must=[FieldCondition(key="section", match=MatchValue(value=section))],
        should=[FieldCondition(key="finding_id", match=MatchValue(value=fid)) for fid in finding_ids]
    )

    # 3. ê²€ìƒ‰ ì„¤ì •
    search_config = HybridSearchConfig(
        es_index="chunks",
        qdrant_collection=config.qdrant_collection_chunks,
        es_top_k=config.chunks_top_k_es,
        vec_top_k=config.chunks_top_k_vec,
        rrf_k=60,
        score_threshold=config.qdrant_score_threshold
    )

    # 4. ê²°ê³¼ ë§¤í•‘ í•¨ìˆ˜
    def map_to_chunk_hit(hit: Dict) -> ChunkHit:
        source = hit.get("_source", {})
        from_qdrant = False

        if not source and "vec_hit" in hit:
            source = hit["vec_hit"].payload
            from_qdrant = True

        # Qdrant payloadì— text ì—†ìœ¼ë©´ ESì—ì„œ ê°€ì ¸ì˜¤ê¸°
        text_content = source.get("text", "")
        if from_qdrant and (not text_content or len(text_content) < 10):
            chunk_id = source.get("chunk_id", hit.get("_id"))
            text_content = self._fetch_text_from_es(chunk_id)
            source["text"] = text_content

        return ChunkHit(
            chunk_id=source.get("chunk_id", hit["_id"]),
            finding_id=source.get("finding_id", ""),
            doc_id=source.get("doc_id", ""),
            section=source.get("section", section),
            section_order=source.get("section_order", 0),
            chunk_order=source.get("chunk_order", 0),
            code=source.get("code"),
            item=source.get("item"),
            item_norm=source.get("item_norm"),
            page=source.get("page"),
            start_line=source.get("start_line"),
            end_line=source.get("end_line"),
            text=text_content,
            text_norm=source.get("text_norm"),
            score_combined=hit.get("rrf_score", 0.0)
        )

    # âœ… ê³µí†µ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë©”ì„œë“œ í˜¸ì¶œ
    chunks = self._hybrid_search(
        query=query,
        es_query=es_query,
        search_config=search_config,
        qdrant_filter=qdrant_filter,
        result_mapper=map_to_chunk_hit,
        use_vector=True,
        top_n=top_n
    )

    return chunks


def _fetch_text_from_es(self, chunk_id: str) -> str:
    """ESì—ì„œ ì²­í¬ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (í—¬í¼ ë©”ì„œë“œ)"""
    try:
        es_doc = self.es.get(index="chunks", id=chunk_id, _source=["text"])
        return es_doc["_source"].get("text", "")
    except ESNotFoundError:
        logger.warning(f"ì²­í¬ {chunk_id}ë¥¼ ESì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return ""
    except ElasticsearchException as e:
        logger.error(f"ESì—ì„œ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {chunk_id} - {e}")
        return ""
```

---

## ë¦¬íŒ©í† ë§ íš¨ê³¼

### ì½”ë“œ ë¼ì¸ ìˆ˜ ë¹„êµ

| í•­ëª© | ê¸°ì¡´ | ë¦¬íŒ©í† ë§ í›„ | ê°ì†Œìœ¨ |
|------|------|-------------|--------|
| **retrieve_findings()** | 213ì¤„ | 80ì¤„ | **62% ê°ì†Œ** |
| **retrieve_chunks_by_section()** | 110ì¤„ | 55ì¤„ | **50% ê°ì†Œ** |
| **ê³µí†µ ë©”ì„œë“œ (_hybrid_search)** | 0ì¤„ | 100ì¤„ | ì‹ ê·œ |
| **í—¬í¼ ë©”ì„œë“œ** | 0ì¤„ | 60ì¤„ | ì‹ ê·œ |
| **ì´ê³„** | 323ì¤„ | **295ì¤„** | **9% ê°ì†Œ** |

### ìœ ì§€ë³´ìˆ˜ì„± ê°œì„ 

| ì¸¡ë©´ | ê¸°ì¡´ | ë¦¬íŒ©í† ë§ í›„ |
|------|------|-------------|
| **ë²„ê·¸ ìˆ˜ì •** | 2ê³³ ìˆ˜ì • | **1ê³³ë§Œ ìˆ˜ì •** âœ… |
| **ê¸°ëŠ¥ ì¶”ê°€** | 2ê³³ ì¶”ê°€ | **1ê³³ë§Œ ì¶”ê°€** âœ… |
| **í…ŒìŠ¤íŠ¸** | 2ê°œ í…ŒìŠ¤íŠ¸ | **1ê°œ í…ŒìŠ¤íŠ¸** âœ… |
| **ê°€ë…ì„±** | ì¤‘ë³µìœ¼ë¡œ í˜¼ë€ | **ëª…í™•í•œ ì—­í• ** âœ… |

---

## ì¶”ê°€ ë¦¬íŒ©í† ë§ ê¸°íšŒ

### 1. í•„í„° êµ¬ì„± ë¡œì§ ê³µí†µí™”

```python
class FilterBuilder:
    """ES/Qdrant í•„í„° ë¹Œë”"""

    @staticmethod
    def build_es_filters(filters: Optional[Dict]) -> List[Dict]:
        """ES í•„í„° ìƒì„±"""
        clauses = []
        if not filters:
            return clauses

        if filters.get("code"):
            clauses.append({"terms": {"code": filters["code"]}})
        if filters.get("doc_id"):
            clauses.append({"terms": {"doc_id": filters["doc_id"]}})
        if filters.get("industry_sub"):
            clauses.append({"terms": {"industry_sub": filters["industry_sub"]}})

        return clauses

    @staticmethod
    def build_qdrant_filter(filters: Optional[Dict]) -> Optional[Filter]:
        """Qdrant í•„í„° ìƒì„±"""
        if not filters:
            return None

        conditions = []
        if filters.get("code"):
            for code in filters["code"]:
                conditions.append(FieldCondition(key="code", match=MatchValue(value=code)))

        if filters.get("doc_id"):
            for doc_id in filters["doc_id"]:
                conditions.append(FieldCondition(key="doc_id", match=MatchValue(value=doc_id)))

        return Filter(should=conditions) if conditions else None
```

### 2. ê²°ê³¼ ë§¤í•‘ í´ë˜ìŠ¤ íŒ¨í„´

```python
from abc import ABC, abstractmethod

class ResultMapper(ABC):
    """ê²°ê³¼ ë§¤í•‘ ì¶”ìƒ í´ë˜ìŠ¤"""

    @abstractmethod
    def map(self, hit: Dict) -> Any:
        """ê²°ê³¼ë¥¼ ë°ì´í„° í´ë˜ìŠ¤ë¡œ ë§¤í•‘"""
        pass


class FindingMapper(ResultMapper):
    """FindingHit ë§¤í¼"""

    def map(self, hit: Dict) -> FindingHit:
        source = self._extract_source(hit)
        score = hit.get("rrf_score", hit.get("_score", 0.0))

        return FindingHit(
            finding_id=source.get("finding_id", hit["_id"]),
            doc_id=source.get("doc_id", ""),
            item=source.get("item"),
            item_detail=source.get("item_detail"),
            code=source.get("code"),
            score_combined=score
        )

    @staticmethod
    def _extract_source(hit: Dict) -> Dict:
        """ES ë˜ëŠ” Qdrant ì†ŒìŠ¤ ì¶”ì¶œ"""
        source = hit.get("_source", {})
        if not source and "vec_hit" in hit:
            source = hit["vec_hit"].payload
        return source


class ChunkMapper(ResultMapper):
    """ChunkHit ë§¤í¼"""

    def __init__(self, es_client: Elasticsearch):
        self.es = es_client

    def map(self, hit: Dict) -> ChunkHit:
        source = self._extract_source(hit)

        # Text fallback ë¡œì§
        text_content = self._get_text_with_fallback(hit, source)

        return ChunkHit(
            chunk_id=source.get("chunk_id", hit["_id"]),
            finding_id=source.get("finding_id", ""),
            # ... í•„ë“œ ë§¤í•‘
            text=text_content,
            score_combined=hit.get("rrf_score", 0.0)
        )

    def _get_text_with_fallback(self, hit: Dict, source: Dict) -> str:
        """Qdrantì— text ì—†ìœ¼ë©´ ESì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
        text = source.get("text", "")
        if text and len(text) >= 10:
            return text

        # ES fallback
        chunk_id = source.get("chunk_id", hit.get("_id"))
        try:
            es_doc = self.es.get(index="chunks", id=chunk_id, _source=["text"])
            return es_doc["_source"].get("text", "")
        except Exception:
            return ""
```

---

## ìµœì¢… ì•„í‚¤í…ì²˜

```
HybridRetriever
â”œâ”€â”€ _hybrid_search()           # ê³µí†µ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
â”‚   â”œâ”€â”€ ES ê²€ìƒ‰
â”‚   â”œâ”€â”€ Qdrant ê²€ìƒ‰
â”‚   â”œâ”€â”€ RRF ìœµí•©
â”‚   â””â”€â”€ ê²°ê³¼ ë§¤í•‘
â”‚
â”œâ”€â”€ retrieve_findings()        # Findings ê²€ìƒ‰
â”‚   â”œâ”€â”€ _filter_docs_by_keywords()
â”‚   â”œâ”€â”€ _build_findings_query()
â”‚   â””â”€â”€ _hybrid_search() í˜¸ì¶œ âœ…
â”‚
â”œâ”€â”€ retrieve_chunks_by_section()  # Chunks ê²€ìƒ‰
â”‚   â”œâ”€â”€ _build_chunks_query()
â”‚   â””â”€â”€ _hybrid_search() í˜¸ì¶œ âœ…
â”‚
â””â”€â”€ í—¬í¼ ë©”ì„œë“œ
    â”œâ”€â”€ FilterBuilder
    â”œâ”€â”€ FindingMapper
    â””â”€â”€ ChunkMapper
```

---

## êµ¬í˜„ ìš°ì„ ìˆœìœ„

### ğŸ”´ High Priority (ì¦‰ì‹œ ì ìš©)
1. **_hybrid_search() ë©”ì„œë“œ ì¶”ì¶œ** - í•µì‹¬ ì¤‘ë³µ ì œê±°

### ğŸŸ¡ Medium Priority (1ì£¼ ë‚´)
2. **ì¿¼ë¦¬ ë¹Œë” ë©”ì„œë“œ ë¶„ë¦¬** - ê°€ë…ì„± í–¥ìƒ
3. **í•„í„° ë¹Œë” í´ë˜ìŠ¤** - í•„í„° ë¡œì§ í†µí•©

### ğŸŸ¢ Low Priority (ì¥ê¸°)
4. **ê²°ê³¼ ë§¤í¼ í´ë˜ìŠ¤ íŒ¨í„´** - OOP ì„¤ê³„ ê°œì„ 
5. **ì „ëµ íŒ¨í„´ ì ìš©** - ë‹¤ì–‘í•œ ê²€ìƒ‰ ì „ëµ ì§€ì›
